{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1] What is a Support Vector Machine (SVM), and how does it work?\n",
        "- A Support Vector Machine (SVM) is a supervised learning algorithm mainly used for classification. It works by finding the optimal hyperplane that separates data into classes with the maximum margin, relying only on key data points called support vectors. If data is not linearly separable, SVM uses kernel functions like polynomial or RBF to transform it into a higher-dimensional space where separation is possible. Its strengths are accuracy and effectiveness in high-dimensional data, though it can be computationally heavy and requires careful parameter tuning."
      ],
      "metadata": {
        "id": "7QoT4Xav106g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2] Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "- In a Hard Margin SVM, the algorithm tries to find a hyperplane that perfectly separates the data into two classes without allowing any misclassification. This approach works only when the data is linearly separable and there is a clear gap between the classes. While it ensures a strict separation, it is very sensitive to noise and outliers, because even a single wrongly placed point can make perfect separation impossible.\n",
        "\n",
        "- A Soft Margin SVM, on the other hand, allows some misclassifications in order to find a balance between maximizing the margin and minimizing classification errors. This flexibility is controlled by a parameter (usually called\n",
        "C), which determines how much penalty is given to misclassified points. A higher C tries to reduce misclassification, potentially leading to overfitting, while a lower C allows more errors but may generalize better. Because real-world data is rarely perfectly separable, Soft Margin SVM is more commonly used than Hard Margin SVM."
      ],
      "metadata": {
        "id": "QNwHfdSo2T7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3] What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case\n",
        "- The kernel trick in SVM is a mathematical technique that allows the algorithm to handle data that is not linearly separable in its original space. Instead of explicitly transforming the data into a higher-dimensional space, the kernel trick uses kernel functions to compute the similarity between data points as if they were mapped to that higher-dimensional space. This makes it possible for SVM to construct a decision boundary in complex scenarios without the heavy computation that comes with directly performing the transformation.\n",
        "\n",
        "One common example is the Radial Basis Function (RBF) kernel, also known as the Gaussian kernel. The RBF kernel maps data into an infinite-dimensional space, making it particularly powerful for handling non-linear relationships. Its use case is in situations where class boundaries are curved or irregular rather than straight lines. For example, if you have data points arranged in concentric circles belonging to different classes, a linear hyperplane cannot separate them, but the RBF kernel can transform the data into a space where they become separable by a simple hyperplane. This makes it one of the most widely used kernels in real-world SVM applications."
      ],
      "metadata": {
        "id": "PrCxe6a92qyQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4] What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
        "- A Naïve Bayes Classifier is a probabilistic machine learning algorithm based on Bayes’ theorem, which calculates the probability of a class given a set of features. It is commonly used for classification tasks such as spam detection, sentiment analysis, and text categorization. The model works by estimating the likelihood of each class based on the individual features of the data, then predicting the class with the highest probability.\n",
        "\n",
        "It is called “naïve” because it makes a strong simplifying assumption: that all features are conditionally independent of each other given the class label. In reality, features often have correlations, but the algorithm ignores them for the sake of simplicity. Despite this unrealistic assumption, Naïve Bayes performs surprisingly well in many practical applications, especially with high-dimensional data such as text, where the independence assumption is “good enough” to provide fast and accurate classification."
      ],
      "metadata": {
        "id": "y3jj8pgM28Ny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5] Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants.\n",
        "When would you use each one?\n",
        "- Gaussian Naïve Bayes is used when the features are continuous and follow a normal (Gaussian) distribution. It models each feature by estimating its mean and variance for every class and then uses these to calculate probabilities. A common use case is in medical data analysis, where measurements like blood pressure, weight, or height are continuous values that can reasonably be assumed to follow a bell-shaped distribution.\n",
        "\n",
        "Multinomial Naïve Bayes is designed for discrete features, particularly those representing counts. Instead of modeling data with a continuous distribution, it calculates the probability of features as frequencies. This makes it very popular in text classification tasks such as spam detection or topic modeling, where features often represent the number of times a word appears in a document.\n",
        "\n",
        "Bernoulli Naïve Bayes, on the other hand, is used when features are binary, meaning they take only two values such as 0 or 1. It is especially useful when the presence or absence of a feature matters more than its frequency. For instance, in document classification, Bernoulli Naïve Bayes would consider whether a word appears in a text at all, rather than how many times it appears, which is useful when word occurrence itself is more informative than word count."
      ],
      "metadata": {
        "id": "ZZXlpFQw3FJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' 6] Write a Python program to:\n",
        "● Load the Iris dataset\n",
        "● Train an SVM Classifier with a linear kernel\n",
        "● Print the model's accuracy and support vectors.\n",
        "'''\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM Classifier with linear kernel\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# Print support vectors\n",
        "print(\"Support Vectors:\\n\", svm_model.support_vectors_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhi4Qkut3RsF",
        "outputId": "61dfe8d3-4c15-44f6-c326-9853e55f7367"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Support Vectors:\n",
            " [[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' 7] Write a Python program to:\n",
        "● Load the Breast Cancer dataset\n",
        "● Train a Gaussian Naïve Bayes model\n",
        "● Print its classification report including precision, recall, and F1-score.\n",
        "'''\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = datasets.load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Gaussian Naïve Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=breast_cancer.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLprvzS33qRo",
        "outputId": "83b65dda-aba9-4de8-9167-29c004acaffd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.93      0.90      0.92        63\n",
            "      benign       0.95      0.96      0.95       108\n",
            "\n",
            "    accuracy                           0.94       171\n",
            "   macro avg       0.94      0.93      0.94       171\n",
            "weighted avg       0.94      0.94      0.94       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' 8] Write a Python program to:\n",
        "● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best C and gamma.\n",
        "● Print the best hyperparameters and accuracy\n",
        "'''\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Define parameter grid for C and gamma\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [0.001, 0.01, 0.1, 1],\n",
        "    'kernel': ['rbf']\n",
        "}\n",
        "\n",
        "# Train SVM using GridSearchCV\n",
        "grid = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best hyperparameters\n",
        "print(\"Best Hyperparameters:\", grid.best_params_)\n",
        "\n",
        "# Predict with best model\n",
        "y_pred = grid.best_estimator_.predict(X_test)\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEKxARzV5L3K",
        "outputId": "b314655d-fc9a-4ede-b766-c10a1ff551df"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Test Accuracy: 0.7777777777777778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''9] Write a Python program to:\n",
        "● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "● Print the model's ROC-AUC score for its predictions.\n",
        "'''\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load the dataset (binary classification: 'sci.space' vs 'rec.autos')\n",
        "categories = ['sci.space', 'rec.autos']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories)\n",
        "\n",
        "X = newsgroups.data\n",
        "y = newsgroups.target\n",
        "\n",
        "# Convert text to TF-IDF features\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_tfidf, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Naïve Bayes Classifier\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_prob = nb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "print(\"ROC-AUC Score:\", roc_auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wVWz5MH7ECv",
        "outputId": "3903a0f6-3ab6-4007-ffbd-87098a7115b9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9993878175696358\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' 10] Imagine you’re working as a data scientist for a company that handles\n",
        "email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        "● Text with diverse vocabulary\n",
        "● Potential class imbalance (far more legitimate emails than spam)\n",
        "● Some incomplete or missing data\n",
        "Explain the approach you would take to:\n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "● Address class imbalance\n",
        "● Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.\n",
        "'''\n",
        "\n",
        "\n",
        "'''Approach to Spam Classification\n",
        "\n",
        "1. Preprocessing the data\n",
        "\n",
        "Clean text (lowercasing, removing HTML tags, special characters, and stopwords).\n",
        "\n",
        "Use TF-IDF vectorization to represent text, since it emphasizes important words over frequent ones.\n",
        "\n",
        "Handle missing or incomplete data by dropping rows with no usable content or replacing with placeholders like \"missing\".\n",
        "\n",
        "2. Model Choice\n",
        "\n",
        "Naïve Bayes: Fast, efficient, and works well with high-dimensional text data.\n",
        "\n",
        "SVM (with linear kernel): More powerful for complex vocabulary and subtle word patterns, often providing higher accuracy.\n",
        "\n",
        "Decision: Start with SVM for accuracy, benchmark against Naïve Bayes for speed.\n",
        "\n",
        "3. Handling Class Imbalance\n",
        "\n",
        "Use class weights in SVM to penalize misclassification of minority class (spam).\n",
        "\n",
        "Optionally apply resampling techniques like SMOTE (oversampling) or undersampling.\n",
        "\n",
        "4. Evaluation Metrics\n",
        "\n",
        "Go beyond accuracy since imbalance can make accuracy misleading.\n",
        "\n",
        "Use Precision, Recall, F1-score, ROC-AUC.\n",
        "\n",
        "Recall is especially critical, since missing spam (false negatives) is worse for business impact.\n",
        "\n",
        "5. Business Impact\n",
        "\n",
        "Improves user trust by reducing the chance of spam reaching inboxes.\n",
        "\n",
        "Saves employee time by filtering junk emails automatically.\n",
        "\n",
        "Protects from phishing and malware, reducing financial and reputational risks '''\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "# 1. Load synthetic email-like dataset (spam-like categories vs not spam)\n",
        "categories = ['sci.space', 'rec.autos']  # spam-like vs non-spam-like\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# 2. Handle missing values by replacing empty strings\n",
        "X = [\"missing\" if text.strip() == \"\" else text for text in X]\n",
        "\n",
        "# 3. Vectorize text with TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# 4. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# 5. Train SVM with class imbalance handling\n",
        "svm_model = SVC(kernel='linear', class_weight='balanced', probability=True, random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# 6. Predictions\n",
        "y_pred = svm_model.predict(X_test)\n",
        "y_prob = svm_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# 7. Evaluation\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=data.target_names))\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_prob))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpuviDFe7k9j",
        "outputId": "5817ea19-d32e-454d-91b1-0e8d23783188"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   rec.autos       0.92      0.93      0.92       297\n",
            "   sci.space       0.93      0.92      0.92       297\n",
            "\n",
            "    accuracy                           0.92       594\n",
            "   macro avg       0.92      0.92      0.92       594\n",
            "weighted avg       0.92      0.92      0.92       594\n",
            "\n",
            "ROC-AUC Score: 0.965525059801154\n"
          ]
        }
      ]
    }
  ]
}