{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1] What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
        "- Ensemble learning in machine learning is the process of combining multiple individual models, often called “weak learners,” to create a single stronger and more accurate predictive model. The key idea is that while one model might make mistakes or overfit to certain aspects of the data, combining several models balances out their weaknesses and improves generalization.\n",
        "\n",
        "There are different ways ensembles can be built. In bagging, multiple models are trained independently on different random subsets of the data, and their predictions are averaged or voted on; Random Forest is a well-known example. In boosting, models are trained sequentially, where each new model focuses on correcting the errors of the previous ones, as in AdaBoost or XGBoost. In stacking, different models are trained and then combined using another model (a “meta-learner”) that learns how best to weigh their predictions.\n",
        "\n",
        "The strength of ensemble learning lies in the principle that “a group of diverse weak models can outperform a single strong one,” provided the models are diverse enough and their errors are not too correlated. This makes ensemble methods some of the most powerful tools in machine learning, widely used in practice and competitions.\n"
      ],
      "metadata": {
        "id": "XHs84YNMSr0C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2] What is the difference between Bagging and Boosting?\n",
        "- Bagging (Bootstrap Aggregating) trains multiple models independently on different random subsets of the training data, created by sampling with replacement. Each model votes (for classification) or averages predictions (for regression). The goal is to reduce variance and avoid overfitting by smoothing out the noise from individual models. Random Forest is the classic example of bagging.\n",
        "\n",
        "Boosting, on the other hand, builds models sequentially. Each new model pays special attention to the errors made by the previous models, giving more weight to misclassified examples. The final prediction is a weighted combination of all models. This process reduces both bias and variance, but because it focuses on mistakes, boosting can be more prone to overfitting if not carefully tuned. AdaBoost, Gradient Boosting, and XGBoost are well-known boosting methods."
      ],
      "metadata": {
        "id": "hPFpSUC0TAm0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3] What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "- Bootstrap sampling is a statistical technique where we create new datasets by randomly sampling from the original dataset with replacement. Each bootstrap sample has the same size as the original dataset, but because of replacement, some data points appear multiple times while others may be left out. On average, about 63% of the original data points appear in a bootstrap sample, while the remaining ~37% are left out, known as the out-of-bag (OOB) samples.\n",
        "\n",
        "In Bagging methods like Random Forest, bootstrap sampling plays a crucial role. Each decision tree in the forest is trained on a different bootstrap sample of the training data. This randomness ensures that the trees are diverse, meaning they won’t all make the same errors. When the predictions of all trees are combined (via majority voting for classification or averaging for regression), the ensemble is more stable and accurate than any single tree."
      ],
      "metadata": {
        "id": "bO3YBW2eTHVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4] What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
        "- Out-of-Bag (OOB) samples are the data points that are not included in a bootstrap sample when training a model in bagging methods like Random Forest. Since bootstrap sampling is done with replacement, on average about 63% of the original data is included in each sample, and the remaining ~37% is left out. These left-out data points are the OOB samples for that particular model.\n",
        "\n",
        "The OOB score uses these samples to evaluate the performance of the ensemble without needing a separate validation set. Specifically, for each data point, we collect predictions from all the models (trees) for which that point was OOB, and then aggregate those predictions to determine how well the ensemble predicts that point. By averaging over all data points, we obtain the OOB score, which is essentially an unbiased estimate of the model’s accuracy (for classification) or error (for regression)."
      ],
      "metadata": {
        "id": "x0k5ZSaUTVbT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5] Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
        "- In a single Decision Tree, feature importance is determined by how much each feature contributes to reducing impurity (like Gini impurity or entropy for classification, or variance for regression) across the splits where it is used. The more a feature reduces impurity and the earlier it appears in the tree, the more important it is considered. However, because a single tree can be unstable and heavily influenced by the training data, its feature importance scores may not be reliable or generalizable.\n",
        "\n",
        "In a Random Forest, feature importance is averaged over many trees, each trained on different bootstrap samples and using random feature subsets at each split. This aggregation smooths out noise and biases from individual trees, giving more stable and robust importance scores. Random Forests also help mitigate the risk of overestimating the importance of certain dominant features because the random feature selection forces the model to explore different variables."
      ],
      "metadata": {
        "id": "TTP5igj8ThID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' 6] Write a Python program to:\n",
        "● Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "● Train a Random Forest Classifier\n",
        "● Print the top 5 most important features based on feature importance scores.\n",
        "'''\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# Sort by importance\n",
        "top_features = feature_importance_df.sort_values(by='Importance', ascending=False).head(5)\n",
        "\n",
        "# Print top 5 features\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(top_features.to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpHY1I2BTwDS",
        "outputId": "e509820b-5623-41f9-843d-5f6a77e90a48"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "             Feature  Importance\n",
            "          worst area    0.139357\n",
            "worst concave points    0.132225\n",
            " mean concave points    0.107046\n",
            "        worst radius    0.082848\n",
            "     worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''7] Write a Python program to:\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "● Evaluate its accuracy and compare with a single Decision Tree\n",
        "'''\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "dt_acc = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "# Bagging with Decision Trees\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bag = bagging.predict(X_test)\n",
        "bagging_acc = accuracy_score(y_test, y_pred_bag)\n",
        "\n",
        "# Print results\n",
        "print(f\"Decision Tree Accuracy: {dt_acc:.4f}\")\n",
        "print(f\"Bagging Classifier Accuracy: {bagging_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQmJmfrAT8mF",
        "outputId": "633e02ad-fbd7-4f79-8300-0674c8619dca"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 0.9333\n",
            "Bagging Classifier Accuracy: 0.9333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' 8]: Write a Python program to:\n",
        "● Train a Random Forest Classifier\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "● Print the best parameters and final accuracy\n",
        "'''\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Define model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 5, 10, 20]\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate on test data\n",
        "y_pred = best_model.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(f\"Final Accuracy on Test Set: {acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9pX8OyDUnXx",
        "outputId": "f32ce233-c853-4300-e259-06619009d96a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 100}\n",
            "Final Accuracy on Test Set: 0.9357\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' 9] Write a Python program to:\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "● Compare their Mean Squared Errors (MSE)\n",
        "'''\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Bagging Regressor with Decision Trees\n",
        "bagging = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bag = bagging.predict(X_test)\n",
        "mse_bag = mean_squared_error(y_test, y_pred_bag)\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=50,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "# Print results\n",
        "print(f\"Bagging Regressor MSE: {mse_bag:.4f}\")\n",
        "print(f\"Random Forest Regressor MSE: {mse_rf:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlhVrAPgU-zr",
        "outputId": "8af31f71-0e93-44f4-9ef4-fe0e581accc2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 0.2579\n",
            "Random Forest Regressor MSE: 0.2577\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10] You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "● Choose between Bagging or Boosting\n",
        "● Handle overfitting\n",
        "● Select base models\n",
        "● Evaluate performance using cross-validation\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        "\n",
        "- Step-by-step approach for loan default prediction using ensemble methods:\n",
        "\n",
        "1) Choose between Bagging or Boosting\n",
        "\n",
        " - Start with both and compare results.\n",
        "\n",
        " - Use Bagging (Random Forest) if variance/overfitting is the main issue.\n",
        "\n",
        " - Use Boosting (XGBoost, AdaBoost, LightGBM) if the model has high bias or needs better handling of subtle patterns.\n",
        "\n",
        "2) Handle Overfitting\n",
        "\n",
        " - Limit tree depth and number of features considered.\n",
        "\n",
        " - Apply regularization (learning rate, L1/L2 penalties in Boosting).\n",
        "\n",
        " - Use early stopping based on validation score.\n",
        "\n",
        " - For Bagging, increase the number of estimators but keep trees shallow.\n",
        "\n",
        "3) Select Base Models\n",
        "\n",
        " - Decision Trees as the primary weak learners.\n",
        "\n",
        " - Try logistic regression or shallow models if trees don’t capture the data well.\n",
        "\n",
        "4) Evaluate Performance (Cross-Validation)\n",
        "\n",
        " - Use stratified k-fold cross-validation to preserve class balance.\n",
        "\n",
        " - Focus on metrics beyond accuracy: AUC-ROC, precision, recall, and F1-score.\n",
        "\n",
        " - Pay special attention to recall, since missing defaulters is costlier than flagging safe borrowers.\n",
        "\n",
        "5) Business Justification\n",
        "\n",
        " - Ensembles reduce errors by combining multiple models.\n",
        "\n",
        " - Improve credit risk detection with fewer false negatives (risky borrowers approved) and false positives (safe borrowers rejected).\n",
        "\n",
        " - Leads to lower financial losses, better decision-making, and stronger customer trust."
      ],
      "metadata": {
        "id": "WoJRZbVzVYRp"
      }
    }
  ]
}