{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1] What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "- A Decision Tree is a simple, tree-like model that helps make decisions by splitting data into smaller and smaller groups based on feature values.\n",
        "How it works in classification-\n",
        "\n",
        "1) Start at the root node\n",
        "\n",
        "- All data is in one place.\n",
        "\n",
        "- The algorithm chooses the best feature to split the data (e.g., \"Age\" or \"Income\") using metrics like Gini Impurity or Entropy (Information Gain).\n",
        "\n",
        "2) Split the data\n",
        "\n",
        "- Based on the chosen feature, the data is divided into branches.\n",
        "\n",
        "Example: If \"Age ≤ 30\" → go left branch, else → right branch.\n",
        "\n",
        "3) Repeat for each branch\n",
        "\n",
        "- Keep splitting until:\n",
        "\n",
        "- The node is pure (all samples belong to one class), or\n",
        "\n",
        "- A stopping criterion is met (e.g., max depth reached).\n",
        "\n",
        "4) Assign a class at the leaf node\n",
        "\n",
        "- When no more splitting is done, the node becomes a leaf and is labeled with the most common class in that group."
      ],
      "metadata": {
        "id": "SSAX5vuyj2ua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2] Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "- Gini Impurity: Measures how often a randomly chosen sample would be misclassified if labels were assigned based on class proportions.\n",
        "\n",
        "              Gini=1−∑pᵢ²\n",
        "\n",
        "0 = pure, higher = more mixed.\n",
        "\n",
        "Entropy: Measures the level of disorder or unpredictability.\n",
        "\n",
        "              Entropy= −∑pᵢ(​log₂​pᵢ)\n",
        "\n",
        "Impact on splits: Decision trees try all possible splits and pick the one that gives the largest drop in impurity (Gini Gain or Information Gain). Gini is faster, Entropy is more information-theoretic, but both usually give similar trees."
      ],
      "metadata": {
        "id": "ik-udPdLkvAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3] What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        "- Pre-pruning: Stops tree growth early (e.g., max depth, min samples split).\n",
        " - How: The tree stops splitting when certain conditions are met (max depth, min samples at a node, min impurity decrease).\n",
        "\n",
        " - Effect: Prevents the tree from becoming too complex from the start.\n",
        "\n",
        " - Example: Stop splitting if a node has fewer than 10 samples.\n",
        "\n",
        " - Advantage: Faster training and less overfitting risk without building unnecessary branches.\n",
        "\n",
        "- Post-pruning: Grows full tree first, then removes weak branches.\n",
        "\n",
        " - How: The tree grows to full depth, then weak/insignificant branches are cut back based on performance on a validation set.\n",
        "\n",
        " - Effect: Keeps only branches that meaningfully improve accuracy.\n",
        "\n",
        " - Example: Build full tree, then remove a split that increases validation error.\n",
        "\n",
        " - Advantage: Finds a better complexity–accuracy balance because it starts from maximum detail and trims."
      ],
      "metadata": {
        "id": "wKLo3qe-me3I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4] What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "- Information Gain (IG): Measures how much a split reduces impurity (usually using Entropy).\n",
        "\n",
        "              IG= Entropyₚₐᵣₑₙₜ - ∑(nchild​/nparent) ​× Entropycₕᵢₗd\n",
        "\n",
        "Importance:\n",
        "\n",
        " - Higher IG = greater reduction in uncertainty.\n",
        "\n",
        " - Decision trees choose the split with maximum IG because it creates the purest child nodes, improving classification accuracy."
      ],
      "metadata": {
        "id": "pJqKtrA5nbbm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5] What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "- Applications:\n",
        "\n",
        " - Credit scoring and loan approval\n",
        "\n",
        " - Predicting customer churn\n",
        "\n",
        " - Medical diagnosis support\n",
        "\n",
        " - Fraud detection in transactions\n",
        "\n",
        " - Product/service recommendations\n",
        "\n",
        "- Advantages:\n",
        "\n",
        " - Simple and easy to interpret\n",
        "\n",
        " - Handles numerical and categorical features\n",
        "\n",
        " - Minimal preprocessing needed\n",
        "\n",
        " - Transparent decision-making process\n",
        "\n",
        "\n",
        "- Limitations:\n",
        "\n",
        " - Easily overfits without pruning\n",
        "\n",
        " - Sensitive to small changes in data\n",
        "\n",
        " - Can produce biased results with imbalanced datasets"
      ],
      "metadata": {
        "id": "IFtFf4VkormW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#6] Write a Python program to:\n",
        "#● Load the Iris Dataset\n",
        "#● Train a Decision Tree Classifier using the Gini criterion\n",
        "#● Print the model’s accuracy and feature importances\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Decision Tree with Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Feature importances\n",
        "for name, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDQ-po9lpl5Z",
        "outputId": "863361df-0742-4aa2-c66c-426d6bcb0cc6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.00\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7] Write a Python program to:\n",
        "#● Load the Iris Dataset\n",
        "#● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fully-grown tree\n",
        "full_tree = DecisionTreeClassifier(random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "full_pred = full_tree.predict(X_test)\n",
        "full_acc = accuracy_score(y_test, full_pred)\n",
        "\n",
        "# Tree with max_depth=3\n",
        "pruned_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "pruned_tree.fit(X_train, y_train)\n",
        "pruned_pred = pruned_tree.predict(X_test)\n",
        "pruned_acc = accuracy_score(y_test, pruned_pred)\n",
        "\n",
        "# Results\n",
        "print(f\"Accuracy (Full Tree): {full_acc:.2f}\")\n",
        "print(f\"Accuracy (Max Depth=3): {pruned_acc:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOE1eV-trTUR",
        "outputId": "f7d87188-fc9f-4a4f-b872-64a8c97ebdcf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Full Tree): 1.00\n",
            "Accuracy (Max Depth=3): 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8]: Write a Python program to:\n",
        "#● Load the California Housing dataset from sklearn\n",
        "#● Train a Decision Tree Regressor\n",
        "#● Print the Mean Squared Error (MSE) and feature importances\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Decision Tree Regressor\n",
        "reg = DecisionTreeRegressor(random_state=42)\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "# MSE\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "\n",
        "# Feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for name, importance in zip(housing.feature_names, reg.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoE3FC2QrmNp",
        "outputId": "818bfe23-d1bc-4896-f70b-b6e544a80cc8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.50\n",
            "\n",
            "Feature Importances:\n",
            "MedInc: 0.5285\n",
            "HouseAge: 0.0519\n",
            "AveRooms: 0.0530\n",
            "AveBedrms: 0.0287\n",
            "Population: 0.0305\n",
            "AveOccup: 0.1308\n",
            "Latitude: 0.0937\n",
            "Longitude: 0.0829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9] Write a Python program to:\n",
        "#● Load the Iris Dataset\n",
        "#● Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV\n",
        "#● Print the best parameters and the resulting model accuracy\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 3, 4, 5]\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "\n",
        "# Accuracy with best model\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCcOf7STr4Nj",
        "outputId": "ef1dbe8d-e5c6-45ab-a59f-d942716c5738"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10] Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world setting.\n",
        "\n",
        "- As a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease here's how i would appraoch the problem with the dataset:\n",
        "\n",
        "1. Handle Missing Values\n",
        "\n",
        " - Numerical features: Replace missing values with median (robust to outliers).\n",
        "\n",
        " - Categorical features: Replace with most frequent category or create a new category \"Unknown\".\n",
        "\n",
        " - Why: Ensures all rows can be used by the model without introducing bias.\n",
        "\n",
        "2. Encode Categorical Features\n",
        "\n",
        " - One-Hot Encoding: For nominal variables (e.g., blood type).\n",
        "\n",
        " - Ordinal Encoding: For ordered variables (e.g., disease stage).\n",
        "\n",
        " - Use ColumnTransformer to apply different encodings to different columns.\n",
        "\n",
        "3. Train a Decision Tree Model\n",
        "\n",
        " - Split data into train/test (e.g., 80/20).\n",
        "\n",
        " - Use DecisionTreeClassifier(criterion='gini', random_state=42).\n",
        "\n",
        " - Fit the model on the processed training data.\n",
        "\n",
        "4. Tune Hyperparameters\n",
        "\n",
        "- Parameters to tune:\n",
        "\n",
        " - max_depth → controls tree depth\n",
        "\n",
        " - min_samples_split → minimum samples to split a node\n",
        "\n",
        " - min_samples_leaf → minimum samples at a leaf\n",
        "\n",
        " - Use GridSearchCV with cross-validation to find the best combination.\n",
        "\n",
        "5. Evaluate Performance\n",
        "\n",
        "- Metrics:\n",
        "\n",
        " - Accuracy (overall correctness)\n",
        "\n",
        " - Precision & Recall (important in healthcare to avoid false negatives)\n",
        "\n",
        " - F1-score (balance between precision & recall)\n",
        "\n",
        " - ROC-AUC (ability to separate classes)\n",
        "\n",
        " - Evaluate on test set to ensure generalization.\n",
        "\n",
        "6. Business Value\n",
        "\n",
        " - Early detection: Helps doctors flag high-risk patients quickly.\n",
        "\n",
        " - Resource optimization: Prioritize testing for those most likely to have the disease.\n",
        "\n",
        " - Cost savings: Reduces unnecessary medical tests.\n",
        "\n",
        " - Better patient outcomes: Timely interventions can save lives."
      ],
      "metadata": {
        "id": "POGgzGLPtCBh"
      }
    }
  ]
}